# BigData
Scripts and setup files in the context of big data

# TODO
* setup first hadoop basic docker image based on alpine
* configure HDFS 
* configure YARN
* expose ports and forwarding
* add optionally multiply nodes
* test the HDFS with mapreduce

# NewTODO
* add key of office pc for ssh
* test hdfs, hdfs command line
* add another node, 2 data nodes, 1 name node
* write bash script for starting this cluster
* expose ports and test YARN, HDFS and MapReduce
* install HBase and Spark next

# Docker Image Versions
* alpine_x11_base_hadoop3_1_1: Just the Hadoop 3.1.1 and Java 
* Version2: Add Zookeeper
* Version3: Add HBase
* Version4: Add Apache Spark
* Version5: Add Hive
* Version6: Add Pig
* Version7: Add Python, Scala, R
* Version8: Add Zeppelin
* Version9: Add Ambari
